---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Week 1

## What is neaurel network

- Ser på housing prediction. 
- Linear regression kan ses som et simpel nne. 
- **neuron** er binde led mellem input og output, som indeholder viden om
    forholdet mellem de to. 
- Relu: recitified linear unit
- Hvis vi tilføjer flere features, såsom størrelse, bedrooms også videre,
så vil der være flere input. Hvor input kan være forbundet til et neuron. 
- Hver hidden layer indholder en del af x. 

## SUpervised learning with nne.

- Her har vi input, output en applikation. 

## Why is deep learning taking off

- Grundet digitalisere er der kommer meget mere data, som gør nne gør det 
bedre. 
- Desuden er de stærkere computer kræfter en stor del af det. 
- Der er sket et skift fra sigmoud til relu funktioner som en aktivition fkt. 
- Han forklarer at med en nne model starter det med en ide, kode og eksperimentere.
da tingene er hurtigere i dag så vil vi hurtigere kunne bevæge os i en cirkel og
forbedre vores ideer. 

## http://ww1.prweb.com/prfiles/2019/06/10/16367838/MRFC-goldnotext.png

- se senere



# Week 2 logistic regression

## Binary Classification

- Vi kommer til at se på backprogation and forward progation. 
- Logitstic regression. 
- Et eksempel er med katte og hvordan billedes måles i pixel. 
- enkelt træning (x,y). x er en x dimensional matrix og y er 0 eller 1. 
- m træning example. ${x^1,y^1}$
- der er m train og m test. 
- VI vil aggerere til X hvor hver lille x er en traning example. 
- Her kommer vi til at burge shape meget i python. 
- Det er også bedre at have X og Y i kolonner. 
- Yshape(1,m).
- Se siden for en notation guide.

## Logistic regression

- HAr binær udkom og vi kan bruge det til klassifikation. 

## Logistic Regression Cost Function

- cost funktionen er sigmoid funktionen. 
- vi har supercript (i) som er opløftet.
- loss error funktion: vi definer en loss funktion. I logistisk er det non konvex.
  Denne funktion bruges til at måle output.
- Cost funktionen måler hvordan vi klare os på hele trænings data.

$$
I(w,b) = \frac{1}{m} \sum^m_{i=i}L(\hat y^{(i), u^{(i)}})
$$
- loss funktionen bruges på en træning, mens cost bruges på alle. 
- Den kan ses som et neural network. 

## Gradient Descent

- En algoritme bruges til at træne parameter i logistisk regression.
- Vi skal finde w, b som min $J(w,b$. 
- Vi differentiere indtli vi når min. 

## Derivatiees

- Gennemgår noget du allerede ved.

## Computation graph

- Vi ser på en computation graph.
- Ser på et mere simpel eksempel end logistisk regression. 
- Vi har J(a,b,c). 
- Den er god når vi har en speciel output variable som vi vil optimerer.
- Det er i det logistiske tilfælde vores cost funktion. 
- Vi opsamler beregnignerne.

## Derivatives with a Computation Graph


## Vectorization


- Er vigtig for at få hurtighed i koden.
- I logistisk regression har vi to vektor w og x. Hvis ikke vi havde en vektor
skulle vi regne det som loops.
- brug  np.dot(w) / w^T, som er hurtigere end loops. 

```{python}
import numpy as np

a = np.array([1,2,3,4])
a
```

```{python}
import time

a = np.random.rand(1000000)
b = np.random.rand(1000000)
tic = time.time()
c = np.dot(a,b)
toc = time.time()
print(c)
print("vectoried veriosn " + str(1000 * (toc - tic)) + "ms" )

# non vektor
c = 0
tic = time.time()
for i in range(1000000):
  c += a[i]*b[i]
toc = time.time()
print(c)
print("non vectoried veriosn " + str(1000 * (toc - tic)) + "ms" )
```

- samme værdi, men non vektor går langsom.

- GPU og CPU. regner på cpu lokalt. Der er simd som betyder vi bruger indbygget instruktioner,
så enable at bruge fordel af computer kraft når vi ikke bruger vektor. 
- Når muligt så undlad brug af loops. 

## Flere eksempler om vectorization

- undgå loops.

```{python}
import numpy as np
u = np.exp(v)
# implicit loop.
#np.log
# etc.
```


## Vectorizing Logistic Regression

- Vi vil vetoisza logistic re.
- brug np.dot til at lave nogel beregninger som gør vi helt skal undgå loops.

## Vectorizing Logistic Regression's Gradient Output

- Ser på vectorization for gradient decent.
- Minder om forrig afsnit.

## Broadcasting in Python

- en teknik for at python løber hurtig.
- Viser kalori eksempel.

## A note on python/numpy vectors

- numpy er en styrke da man opnår fleksibilitet.

```{python}
import numpy as np
a = np.random.randn(5)
print(a)
```
```{python}
a.shape
```
```{python}
a.T
```
- Minder om hinanden. 
- Derfor skal vi ikke bruge data struktur.
- Brug i steder

```{python}
a = np.random.randn(5,1)
a
```
- Læg her mærke til vi får [[]].

```{python}
a = np.random.randn(3, 3)
b = np.random.randn(3, 1)
c = a*b
a
b
c
```

```{python}
a = np.random.randn(3, 3)
b = np.random.randn(3, 1)
c = a*b

c.shape
```


# Week 3

# Week 4

