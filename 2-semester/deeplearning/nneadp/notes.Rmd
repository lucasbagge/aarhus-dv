---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Week 1

## What is neaurel network

- Ser på housing prediction. 
- Linear regression kan ses som et simpel nne. 
- **neuron** er binde led mellem input og output, som indeholder viden om
    forholdet mellem de to. 
- Relu: recitified linear unit
- Hvis vi tilføjer flere features, såsom størrelse, bedrooms også videre,
så vil der være flere input. Hvor input kan være forbundet til et neuron. 
- Hver hidden layer indholder en del af x. 

## SUpervised learning with nne.

- Her har vi input, output en applikation. 

## Why is deep learning taking off

- Grundet digitalisere er der kommer meget mere data, som gør nne gør det 
bedre. 
- Desuden er de stærkere computer kræfter en stor del af det. 
- Der er sket et skift fra sigmoud til relu funktioner som en aktivition fkt. 
- Han forklarer at med en nne model starter det med en ide, kode og eksperimentere.
da tingene er hurtigere i dag så vil vi hurtigere kunne bevæge os i en cirkel og
forbedre vores ideer. 

## http://ww1.prweb.com/prfiles/2019/06/10/16367838/MRFC-goldnotext.png

- se senere



# Week 2 logistic regression

## Binary Classification

- Vi kommer til at se på backprogation and forward progation. 
- Logitstic regression. 
- Et eksempel er med katte og hvordan billedes måles i pixel. 
- enkelt træning (x,y). x er en x dimensional matrix og y er 0 eller 1. 
- m træning example. ${x^1,y^1}$
- der er m train og m test. 
- VI vil aggerere til X hvor hver lille x er en traning example. 
- Her kommer vi til at burge shape meget i python. 
- Det er også bedre at have X og Y i kolonner. 
- Yshape(1,m).
- Se siden for en notation guide.

## Logistic regression

- HAr binær udkom og vi kan bruge det til klassifikation. 

## Logistic Regression Cost Function

- cost funktionen er sigmoid funktionen. 
- vi har supercript (i) som er opløftet.
- loss error funktion: vi definer en loss funktion. I logistisk er det non konvex.
  Denne funktion bruges til at måle output.
- Cost funktionen måler hvordan vi klare os på hele trænings data.

$$
I(w,b) = \frac{1}{m} \sum^m_{i=i}L(\hat y^{(i), u^{(i)}})
$$
- loss funktionen bruges på en træning, mens cost bruges på alle. 
- Den kan ses som et neural network. 

## Gradient Descent

- En algoritme bruges til at træne parameter i logistisk regression.
- Vi skal finde w, b som min $J(w,b$. 
- Vi differentiere indtli vi når min. 

## Derivatiees

- Gennemgår noget du allerede ved.

## Computation graph

- Vi ser på en computation graph.
- Ser på et mere simpel eksempel end logistisk regression. 
- Vi har J(a,b,c). 
- Den er god når vi har en speciel output variable som vi vil optimerer.
- Det er i det logistiske tilfælde vores cost funktion. 
- Vi opsamler beregnignerne.

## Derivatives with a Computation Graph

- 


# Week 3

# Week 4

