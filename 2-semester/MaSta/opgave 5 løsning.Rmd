---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 2.11: Let the random variable X have a Cauchy distribution with pdf f (x) = 1âˆ•(Ï€(1 + (x âˆ’ ğœƒ)2)) for âˆ’âˆ < x < âˆ.

## a) Show that the mean of X does not exist.


## b) Moregenerally,willE[Xk]exist?(k=1,2,3,...). c) Show that ğœƒ is the median of the distribution.

# 4.32: Let X1,X2,...,X10 âˆ¼ Pois(3). Let X = i=1 Xi. Find the pdf for the sampling distribution of X.



# 5.2:  Consider the samples 1, 3, 4, and 6 from some distribution.

## a) For one random bootstrap sample, find the probability that the mean is 1.

## b) For one random bootstrap sample, find the probability that the maximum is 6.

## c) For one random bootstrap sample, find the probability that exactly two elements in the sample are less than 2.


# 6.2: Prove Proposition 6.1.2.

# 6.4: Let X1, X2, ... , Xn be a random sample from the distribution with pdf f (x; ğœƒ) = (x3eâˆ’xâˆ•ğœƒ)âˆ•(6ğœƒ4) for x â‰¥ 0. Calculate the maximum likelihood estimate of ğœƒ.

# 6.12: Let X1, X2 be random sample from N(ğœ‡, 32) and Y1, Y2 be a random sample from N(2ğœ‡,32), and assume the Xâ€²s are independent of the Yâ€²s. ij SupposeX1 =3,X2 =âˆ’1,Y1 =3,Y2 =2.Find the MLE ofğœ‡.


# 5.3:  Consider the samples 1â€“3.

> I 5.3 menes med â€samples 1â€“3â€ at vi ser pa en stikprÃ¸ve sim indeholder vÃ¦rdierne 1,2 Ëš
og 3. Til begge opgaver findes korte svar i bogen (side 516).


## a) List all the (ordered) bootstrap samples from this sample. How many are there?

3 i 3

## b) How many unordered bootstrap samples are there? For example, {1, 2, 2} and {2, 1, 2} are considered to be the same.

unored og brugr 2 a minus n og fÃ¥r 10 forskellige udtryk

## c) How many ordered bootstrap samples have one occurrence of 1 and two occurrences of 3? Is this the same number of bootstrap samples that have each of 1, 2, and 3 occurring exactly once?

 

## d) Is the probability of obtaining a bootstrap sample with one 1 and two 3â€™s the same as the probability of obtaining a bootstrap sample with each of 1, 2, and 3 occurring exactly once?



# 5.5

# 5A Fordelingsmodeller i praksis

Denne opgave omhandler valg af en passende fordelingsmodel. Mulige kandidater er:
familier af binomial-, negative binomial-, geometriske, Poisson, normal-, og eksponentialfordelinger.

## a) I et call center overvages indgÃ¥aende opkald mellem kl. 10:00 og kl. 11:00. Der registreres tidspunkterne, og om kunden var sur eller glad, da vedkommende ringede. I alt besvarede call centret 48 opkald. DesvÃ¦rre kom 32 fra sure kunder. Lad vÃ¦re med at regne - angiv bare, hvilken fordelingsfamilie du vÃ¦lger, for at beskrive

### antallet af indgaende opkald mellem kl. 10:20 og kl. 10:25, Ëš

poisson, da antallet er diskret og tilfÃ¦ldigt fordelt.

### ventetiden mellem fÃ¸rste og andet opkald i tidsrummet 10:00 â€“ 11:00,

eksponential, ventiden mellem to event.

### antal af sure kunder blandt de fÃ¸rste 20 der ringede,

binomial, sur = fiasko, glad = succes

### antal af sure opkald indtil den fÃ¸rste venlige opkald.

geometrisk, antal indtil succes.

## b) Angiv eksempler pa situationer, hvor du ville bruge et af de resterende fordelingsmod eller (ud over dem du har brugt i pkt (a).

Normal kunne beskrive ventetiden mellem all opkald i perioden.
negativ binomial kunne vÃ¦re antall af opkald indtil 4 succeser.  

# 6.10 MLE  

![](6.10.png)

$$
L(\mu|x=95,y=130)=\frac{1}{15\cdot \sqrt(2\pi)}e^{-\frac{(95-\mu)^2}{2*15^2}} \cdot 
\frac{1}{30\cdot \sqrt(2\pi)}e^{-\frac{(130-1.3\cdot \mu)^2}{2*20^2}}
$$

differentiere og sÃ¦t lig med nul og vi fÃ¥r
$$
\mu = 97.44
$$

# 6.16 Let the five numbers 2, 3, 5, 9, 10 come from the uniform distribution on [ğ›¼, ğ›½]. Find the method of moments estimates of ğ›¼ and ğ›½.

vi skal bruge en metode hvor vi regne sample momen og den teoretisk for til
sidst at lÃ¸se lignignerne.

- sample moment

$$
\frac{1}{5}(2 + 3 +5+9+10)=5.8
$$

- sample moment

$$
\frac{1}{5}(2^2 + 3^2 +5^2+9^2+10^2)=43.8
$$

- teoretisk momen

$$
E[X]=\int^B_a x \frac{1}{B-a} dx = \frac{b^2-a^2}{2(b-a)}
$$
- teoretisk moment
$$
E[X]=\int^B_a x^2 \frac{1}{B-a} dx = \frac{b^3-a^3}{4(b-a)}
$$

lÃ¸ser ligningsystem

$$
\frac{b^2-a^2}{2(b-a)}=5.8
$$

$$
\frac{b^3-a^3}{3(b-a)}=54.8

$$
doven og fÃ¥r

$$
\alpha = 0.28 \\ \beta = 11.32
$$

# 6.30 bias estimator

![](6.30.png)

Vi husker at $\bar X$ er en unbiased estimator for mu.
Lad $h(x)=5x^2$. Vi vil gerne vise, at $h(\bar X)$ er en biased estimator for $h(\mu)$.
Vi vil altsÃ¥ vise $E(h(\bar X))\neq h(\mu)$

$$
E(h(\bar X)) =E(5\bar X^2) \\
= 5E(\bar X^2)\\
= 5(var(\bar X)+E(\bar X)^2) \\
= 5(\frac{\sigma^2}{n} + \mu^2)
$$
bruger theorem 5.4 at estimator for stik prÃ¸ve variansen er ovenstÃ¥ene. 
Vi fÃ¥r ikke h(mu) derfor er den biased. 

Vi ser det Ã¸nskede af ovenstÃ¥ende. Vi kan dog bemÃ¦rke at for n -> infty vil 
ovenstÃ¥ende gÃ¥ mod 5mu^2 = h(mu). SÃ¥ asymptotisk er h(bar X) en unbiaed estimator
for h(mu), da sigma2/n hvor n vil gÃ¥ mod nul og derfor fÃ¥r vi 5mu.


# 6.34 mse ad smaller estimator

![](6.34.png)

### a) 

find mindst mse, var + bias^2

udregn bias

$$
B(\hat \theta_1) = \theta - \theta =0 \\
MSE(\hat \theta_1) = 25
$$
$$
B(\hat \theta_2) = \theta + 3 - \theta = 3 \\
MSE(\hat \theta_1) = 4 + 3^2 = 13
$$

theta2 er er med mindst fejl.

### b) 

Her skal vi isoler b

$$
25 > 4+b^2\longrightarrow \\
21>b^2 \longrightarrow \\
0\le b\le\sqrt(21)
$$

# random opgave

```{r}
n <- 16
curve(x*(1-x)/n, from = 0,
      to = 1,
      xlab = "p",
      ylab = "mse")

curve(n*(1-x)*x/(n+2)^2 + (1-2*x)^2/(n+2)^2, add = TRUE, col = "blue", lty=2)
```

nÃ¥r n er 16 er der en forskel pÃ¥ mse, men jo hÃ¸jere vi kommer op desto mere
konvergere den mod den fÃ¸rst sÃ¥ clt.

Er den konsisten? betyder at den ogsÃ¥ konvergere med en hvis sandsynlighed.
Brug her proposition 6.3.4

![](6.3.4.png)

I kapitel 6 som Eskild gennegÃ¥r ser vi bare desto stÃ¸rre n bliver vil vi fÃ¥
den samme fordeling. Husk proposition 6.3.4.


# 5.B: Consider a population that has a normal distribution with mean ğœ‡ = 36, standard deviation ğœ = 8

## a) The sampling distribution of X for samples of size 200 will have what mean, standard error, and shape

## b) Use R to draw a random sample of size 200 from this population. Conduct EDA on your sample

## c) Compute the bootstrap distribution for your sample, and note the bootstrap mean and standard error.

## d) Compare the bootstrap distribution to the theoretical sampling distribution by creating a table like Table 5.2

## e) Repeat for sample sizes of n = 50 and n = 10. Carefully describe your observations about the effects of sample size on the bootstrap distribution


# 5.9: Consider a population that has a gamma distribution with parameters r = 5, ğœ† = 1âˆ•4.

## a) Use simulation (with n = 200) to generate an approximate sampling distribution of the mean; plot and describe the distribution.

```{r}
set.seed(1234)
n = 200
N = 10^4 # 10000 gange
Xbar = numeric(N)

for (i in 1:N){
  sample <- rgamma(n, shape = 5, rate = 1/4)
  Xbar[i] <- mean(sample)
}

mean(Xbar)
```

```{r}
sd(Xbar)
```

```{r}
hist(Xbar)
```

Det ser normal fordelt ud. Det er der et theorem der siger.


## b) Now, draw one random sample of size 200 from this population.Create a histogram of your sample, and find the mean and standard deviation

Tag et eksempel

```{r}
sample <- rgamma(n, 5, 1/4)

mean(sample)
sd(sample)
```

```{r}
hist(sample)
```

Det ser mere ud til at vÃ¦re en gamem fordeling.


## c) Compute the bootstrap distribution of the mean for your sample, plot it, and note the bootstrap mean and standard error.

```{r}
N = 10^4

bootstrapXbar = numeric(N)

for (i in 1:N){
  bootstrap <- sample(sample, n, replace = TRUE)
  bootstrapXbar[i] <- mean(bootstrap)
}
mean(bootstrapXbar)
```

```{r}
sd(bootstrapXbar)
```

Vi tager det ud fra en stikprÃ¸ve og ikke populationen. Det er essens af bootstrap.

```{r}
hist(bootstrapXbar)
```

Vi ser det konvergere mod en normal fordeling. 



## d) Compare the bootstrap distribution to the approximate theoretical sampling distribution by creating a table like Table 5.2.

```{r}
pop_mean <- 5/(1/4)
pop_sd <- sqrt(5/(1/4)^2)
tbl <- matrix(c(pop_mean, 
                pop_sd, 
                mean(Xbar),
                sd(Xbar),
                mean(sample),
                sd(sample),
                mean(bootstrapXbar), 
                sd(bootstrapXbar)),
                nrow = 4,
                byrow = TRUE)

rownames(tbl) <- c("Population", "Sampling distribution of Xbar",
                   "Sample", "Bootstrap districution")
colnames(tbl) <- c("mean", "std")
tbl
```

sÃ¦tter middel vÃ¦rdi og st op mod hianden. 
De minder om hinanden bootstrap, sample og pop. 

## e) Repeat (a)â€“(e) for sample sizes of n = 50 and n = 10. Describe care- fully your observations about the effects of sample size on the bootstrap distribution



# 5.13: The data set FishMercury contains mercury levels (parts per million) for 30 fish caught in lakes in Minnesota.

## a) Create a histogram or boxplot of the data. What do you observe?

```{r}
fish <- read.csv("MatStat-R/data/FishMercury.csv")
boxplot(fish)
```

Der er en outlier.



## b) Bootstrap the mean, and record the bootstrap standard error and the 95% bootstrap percentile interval.

```{r}
set.seed(1234)
data <- fish$Mercury
nsim <- 1000
xbar <- numeric(nsim)

# sample

for (i in 1:nsim) {
  bootdata <- sample(data, replace = TRUE)
  xbar[i] <- mean(bootdata)
}

mean(xbar)
sd(xbar)

quantile(xbar, probs = c(0.025, 0.975))
hist(xbar)
```

vi fÃ¥r standard erro, mean og konfidens bÃ¥nd. SÃ¥ vi ser den rigtig vÃ¦rdi ligger i intervallet. 
VI ser ogsÃ¥ histogrammet

## c) Remove the outlier and bootstrap the mean of the remaining data. Record the bootstrap standard error and the 95% bootstrap percentile interval.

```{r}
library(tidyverse)
library(magrittr)
fish_modified <- fish %>% filter(Mercury < max(Mercury))

boxplot(fish_modified)
```

Den ser pÃ¦ner ud og er mere normal fordelt.

##d) What effect did removing the outlier have on the bootstrap distribu- tion, in particular, the standard error?

```{r}
set.seed(1234)
data <- fish_modified$Mercury
nsim <- 1000
xbar <- numeric(nsim)

# sample

for (i in 1:nsim) {
  bootdata <- sample(data, replace = TRUE)
  xbar[i] <- mean(bootdata)
}

mean(xbar)
sd(xbar)

quantile(xbar, probs = c(0.025, 0.975))
hist(xbar)
```

sd er mindre. Det sammes. 

I bootstrop tager man stikprÃ¸ver af sin stikprÃ¸ver. 



# 6.14 

![](6.14.png)
Vi har at $X_1,..,X_n\sim Gamma(r,\lambda)$
med pdf $f(x)=\frac{\lambda^r}{\Gamma(r)}x^{r-1}e^{-\lambda x}$

Vores likelihood funktion er givet ved

![](6.14lÃ¸snign.png)
gamme fordeling som vi skal finde estimer r og lambda.
vi skal finde log lieklihood estimator.
opstil log likelihood.
nÃ¥r vi har den skal vi pÃ¥ 158 definere for hver parameter og sÃ¦t lig nul
og lÃ¦s ligninsystemet. 
log likeligho er et produkt af pdf. 
Summer er letter at arbejde med. 