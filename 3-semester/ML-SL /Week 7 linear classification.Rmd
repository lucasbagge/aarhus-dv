---
title: "Week 7"
description: |
  A new article created using the Distill format.
author:
  - name: Lucas Bagge
    url: https://example.com/norajones
    affiliation: Spacely Sprockets
    affiliation_url: https://example.com/spacelysprokets
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(dplyr)
library(magrittr)
```

## Data

```{python}
# Dependencies
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA 
import matplotlib.pyplot as plt 
import seaborn as sns; sns.set() # this import just makes the plots prettier
import numpy as np
```

```{python}
# We're using a subset of two classes for now
digits = load_digits(n_class=2)
```

```{python}
# Handy plotting functions
x_min, x_max = -40, 40
y_min, y_max = -40, 40

def plot_examples():
    show_num = 4
    _, axes = plt.subplots(1, show_num)
    images_and_labels = list(zip(digits.images, digits.target))
    for ax, (image, label) in zip(axes[:], images_and_labels[:show_num]):
        ax.set_axis_off()
        ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
        ax.set_title('Label: %i' % label)

def plot_scatter(data, target, n_class=2):
    plt.scatter(data[:, 0], data[:, 1], c=target, edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('rainbow', n_class))
    plt.xlabel('Component 1')
    plt.ylabel('Component 2')
    plt.colorbar();
    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)

def plot_decision_boundary(data, weights):
    raise NotImplementedError("You should fill this is")
```
      
```{python}
plot_examples()
plt.show()
```
      
```{python}
      # The dataset contains 2D data in the form of the `images` attribute, 
      # as well as a 1D-version called `data`, where the images have been flattened. 
np.array_equal(digits.images[0].flatten(), digits.data[0])
```
      
```{python}
# We can get a 2D version of the data using PCA
pca = PCA(n_components=2)
X = pca.fit_transform(digits.data) # this is the representation, we'll be working with
```
      
```{python}
# Out targets are in the set {0,1}
t_01 = digits.target
```
      
```{python}
# Let's plot all the data in 2D
plot_scatter(X, t_01)
plt.show()
```
      
      
## Least squares for classification
      
```{r}
X_r <- py$X
t_01_r <- py$t_01
```
      
```{r}
mydata <- X_r
mydata <- cbind(mydata, C=t_01_r)
head(mydata)
print(class(mydata))
```
      
      
```{r}
compute_W <- function(X, C) {
  solve(t(X) %*% X) %*% t(X) %*% C
}

X <- matrix(cbind(rep(1,length(mydata[,1])), mydata[,1:2]), ncol=3)
C <- matrix(mydata[,3], ncol=1)
W <- compute_W(X, C)
library(glue)
X[1,]
W
```
        
```{r}
# given a W, create a discriminant function defined by it
discriminant_factory <- function(W) {
  function(x) t(W) %*% c(1,x)
  }

f <- discriminant_factory(W)  # this problem's discriminant function
```

```{r}
newdata       <-
  mydata %>%
  as.data.frame() %>%
  dplyr::select(-c(C),
  X1 = V1,
  X2 = V2) %>%
  as.matrix()
```

```{r}
prediction <- ifelse(
  apply(
    newdata, 
    1,
    f) <= 0.5, 
  1, 
  2
) # apply threshold

plot(newdata, col=c("red","blue")[prediction], xlab="X1", ylab="X2", pch=19)

# draw decision line: w0 + w1x1 + w2x2 == 1.5
abline((0.5-W[1])/W[3], -W[2]/W[3], lty=2) 
```




## Perceptron

```{r}
perceptron <- function(X, Y, eta=1, epsilon=1e-3) {
  n <- nrow(X)
  W <- rnorm(ncol(X)+1)  # init values are small random numbers

  for(i in 1:1e4) {
    
    W_start <- W                 # keep W for convergence comparisation
    for (k in 1:n) {             # update W using all samples
      
      x <- c(1,X[k,])            # the data point
      y <- sign(W %*% x)         # what the model estimates for x
      d <- Y[k]                  # what we desire for x
      W <- W + eta * (d-y) * x   # if d-y>0 then an error was found
    }
    
    if (norm(as.matrix(W_start  - W), "F")<epsilon)
      return(W)                  # convergence achieved

  } # for(i)
  
  return(NA)                     # non convergence, give up
}
```


```{r}
classes <- sample(1:2, 100, replace=TRUE)
X <- mydata[,1:2]
Y <- ifelse(mydata[,3]==1,-1,1)  # convert to classes -1 & 1

W <- perceptron(X, Y, eta = 0.1)

plot(X, col=c("red","blue")[classes], xlab="X1", ylab="X2", pch=19)
# draw decision line: w0 + w1x1 + w2x2 == 0
abline(-W[1]/W[3],
       -W[2]/W[3],
       lty=2) 
```

## Logistic regression

```{r}
sigmoid <- function(x) { 
  1 / (1 + exp(-x) ) 
  }
curve(sigmoid, -5, 5, col="red", lwd=2)
```


```{python}
lr = 0.01
threshold = 0.5
num_iter = 100000

def sigmoid(z):
        return 1 / (1 + np.exp(-z))
      
theta = np.zeros(X.shape[1])
z = np.dot(X, theta)
h = sigmoid(z)

gradient = np.dot(X.T, (h - t_01)) / t_01.size

for i in range(num_iter):
  z = np.dot(X, theta)
  h = sigmoid(z)
  gradient = np.dot(X.T, (h - t_01)) / t_01.size
  
  theta -= lr * gradient
  #print(f'loss: {theta} \t')
  
predict_prob_own = sigmoid(np.dot(X, theta))
predict_own = predict_prob_own >= threshold
```


```{python}
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.scatter(X[t_01 == 0][:, 0], X[t_01 == 0][:, 1], color='b', label='0')
plt.scatter(X[t_01 == 1][:, 0], X[t_01 == 1][:, 1], color='r', label='1')
plt.legend()
x1_min, x1_max = X[:,0].min(), X[:,0].max(),
x2_min, x2_max = X[:,1].min(), X[:,1].max(),
xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))
grid = np.c_[xx1.ravel(), xx2.ravel()]
probs = sigmoid(np.dot(grid, theta)).reshape(xx1.shape)
plt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='black');
plt.show()
```
